{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER6. 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 물질의 화학적 구성 판단\n",
    "\n",
    "- 215개 육류 샘플을 100개의 주파에 대해 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) R 을 켜서 데이터를 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(tecator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tecator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- absorp 행렬에는 215개의 샘플에 대한 100개의 흡수율값이 있고,**행렬 끝의 1~3열**에는 각각 **수분, 지방, 단백질 함유량**이 들어 있다.\n",
    "\n",
    "- absorp 행렬은 예측변수, endpoints 행렬이 반응 변수로 쓰일 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>2.61776</td><td>2.61814</td><td>2.61859</td><td>2.61912</td><td>2.61981</td><td>2.62071</td><td>2.62186</td><td>2.62334</td><td>2.62511</td><td>2.62722</td><td>...    </td><td>3.00247</td><td>2.98145</td><td>2.96072</td><td>2.94013</td><td>2.91978</td><td>2.89966</td><td>2.87964</td><td>2.85960</td><td>2.83940</td><td>2.81920</td></tr>\n",
       "\t<tr><td>2.83454</td><td>2.83871</td><td>2.84283</td><td>2.84705</td><td>2.85138</td><td>2.85587</td><td>2.86060</td><td>2.86566</td><td>2.87093</td><td>2.87661</td><td>...    </td><td>3.30473</td><td>3.29186</td><td>3.27921</td><td>3.26655</td><td>3.25369</td><td>3.24045</td><td>3.22659</td><td>3.21181</td><td>3.19600</td><td>3.17942</td></tr>\n",
       "\t<tr><td>2.58284</td><td>2.58458</td><td>2.58629</td><td>2.58808</td><td>2.58996</td><td>2.59192</td><td>2.59401</td><td>2.59627</td><td>2.59873</td><td>2.60131</td><td>...    </td><td>2.70934</td><td>2.68951</td><td>2.67009</td><td>2.65112</td><td>2.63262</td><td>2.61461</td><td>2.59718</td><td>2.58034</td><td>2.56404</td><td>2.54816</td></tr>\n",
       "\t<tr><td>2.82286</td><td>2.82460</td><td>2.82630</td><td>2.82814</td><td>2.83001</td><td>2.83192</td><td>2.83392</td><td>2.83606</td><td>2.83842</td><td>2.84097</td><td>...    </td><td>2.99820</td><td>2.97367</td><td>2.94951</td><td>2.92576</td><td>2.90251</td><td>2.87988</td><td>2.85794</td><td>2.83672</td><td>2.81617</td><td>2.79622</td></tr>\n",
       "\t<tr><td>2.78813</td><td>2.78989</td><td>2.79167</td><td>2.79350</td><td>2.79538</td><td>2.79746</td><td>2.79984</td><td>2.80254</td><td>2.80553</td><td>2.80890</td><td>...    </td><td>3.32201</td><td>3.30025</td><td>3.27907</td><td>3.25831</td><td>3.23784</td><td>3.21765</td><td>3.19766</td><td>3.17770</td><td>3.15770</td><td>3.13753</td></tr>\n",
       "\t<tr><td>3.00993</td><td>3.01540</td><td>3.02086</td><td>3.02634</td><td>3.03190</td><td>3.03756</td><td>3.04341</td><td>3.04955</td><td>3.05599</td><td>3.06274</td><td>...    </td><td>3.57163</td><td>3.55877</td><td>3.54651</td><td>3.53442</td><td>3.52221</td><td>3.50972</td><td>3.49682</td><td>3.48325</td><td>3.46870</td><td>3.45307</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "\t 2.61776 & 2.61814 & 2.61859 & 2.61912 & 2.61981 & 2.62071 & 2.62186 & 2.62334 & 2.62511 & 2.62722 & ...     & 3.00247 & 2.98145 & 2.96072 & 2.94013 & 2.91978 & 2.89966 & 2.87964 & 2.85960 & 2.83940 & 2.81920\\\\\n",
       "\t 2.83454 & 2.83871 & 2.84283 & 2.84705 & 2.85138 & 2.85587 & 2.86060 & 2.86566 & 2.87093 & 2.87661 & ...     & 3.30473 & 3.29186 & 3.27921 & 3.26655 & 3.25369 & 3.24045 & 3.22659 & 3.21181 & 3.19600 & 3.17942\\\\\n",
       "\t 2.58284 & 2.58458 & 2.58629 & 2.58808 & 2.58996 & 2.59192 & 2.59401 & 2.59627 & 2.59873 & 2.60131 & ...     & 2.70934 & 2.68951 & 2.67009 & 2.65112 & 2.63262 & 2.61461 & 2.59718 & 2.58034 & 2.56404 & 2.54816\\\\\n",
       "\t 2.82286 & 2.82460 & 2.82630 & 2.82814 & 2.83001 & 2.83192 & 2.83392 & 2.83606 & 2.83842 & 2.84097 & ...     & 2.99820 & 2.97367 & 2.94951 & 2.92576 & 2.90251 & 2.87988 & 2.85794 & 2.83672 & 2.81617 & 2.79622\\\\\n",
       "\t 2.78813 & 2.78989 & 2.79167 & 2.79350 & 2.79538 & 2.79746 & 2.79984 & 2.80254 & 2.80553 & 2.80890 & ...     & 3.32201 & 3.30025 & 3.27907 & 3.25831 & 3.23784 & 3.21765 & 3.19766 & 3.17770 & 3.15770 & 3.13753\\\\\n",
       "\t 3.00993 & 3.01540 & 3.02086 & 3.02634 & 3.03190 & 3.03756 & 3.04341 & 3.04955 & 3.05599 & 3.06274 & ...     & 3.57163 & 3.55877 & 3.54651 & 3.53442 & 3.52221 & 3.50972 & 3.49682 & 3.48325 & 3.46870 & 3.45307\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 2.61776 | 2.61814 | 2.61859 | 2.61912 | 2.61981 | 2.62071 | 2.62186 | 2.62334 | 2.62511 | 2.62722 | ...     | 3.00247 | 2.98145 | 2.96072 | 2.94013 | 2.91978 | 2.89966 | 2.87964 | 2.85960 | 2.83940 | 2.81920 | \n",
       "| 2.83454 | 2.83871 | 2.84283 | 2.84705 | 2.85138 | 2.85587 | 2.86060 | 2.86566 | 2.87093 | 2.87661 | ...     | 3.30473 | 3.29186 | 3.27921 | 3.26655 | 3.25369 | 3.24045 | 3.22659 | 3.21181 | 3.19600 | 3.17942 | \n",
       "| 2.58284 | 2.58458 | 2.58629 | 2.58808 | 2.58996 | 2.59192 | 2.59401 | 2.59627 | 2.59873 | 2.60131 | ...     | 2.70934 | 2.68951 | 2.67009 | 2.65112 | 2.63262 | 2.61461 | 2.59718 | 2.58034 | 2.56404 | 2.54816 | \n",
       "| 2.82286 | 2.82460 | 2.82630 | 2.82814 | 2.83001 | 2.83192 | 2.83392 | 2.83606 | 2.83842 | 2.84097 | ...     | 2.99820 | 2.97367 | 2.94951 | 2.92576 | 2.90251 | 2.87988 | 2.85794 | 2.83672 | 2.81617 | 2.79622 | \n",
       "| 2.78813 | 2.78989 | 2.79167 | 2.79350 | 2.79538 | 2.79746 | 2.79984 | 2.80254 | 2.80553 | 2.80890 | ...     | 3.32201 | 3.30025 | 3.27907 | 3.25831 | 3.23784 | 3.21765 | 3.19766 | 3.17770 | 3.15770 | 3.13753 | \n",
       "| 3.00993 | 3.01540 | 3.02086 | 3.02634 | 3.03190 | 3.03756 | 3.04341 | 3.04955 | 3.05599 | 3.06274 | ...     | 3.57163 | 3.55877 | 3.54651 | 3.53442 | 3.52221 | 3.50972 | 3.49682 | 3.48325 | 3.46870 | 3.45307 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9]   \n",
       "[1,] 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511\n",
       "[2,] 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093\n",
       "[3,] 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873\n",
       "[4,] 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842\n",
       "[5,] 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553\n",
       "[6,] 3.00993 3.01540 3.02086 3.02634 3.03190 3.03756 3.04341 3.04955 3.05599\n",
       "     [,10]   [,11] [,12]   [,13]   [,14]   [,15]   [,16]   [,17]   [,18]  \n",
       "[1,] 2.62722 ...   3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964\n",
       "[2,] 2.87661 ...   3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659\n",
       "[3,] 2.60131 ...   2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718\n",
       "[4,] 2.84097 ...   2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794\n",
       "[5,] 2.80890 ...   3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766\n",
       "[6,] 3.06274 ...   3.57163 3.55877 3.54651 3.53442 3.52221 3.50972 3.49682\n",
       "     [,19]   [,20]   [,21]  \n",
       "[1,] 2.85960 2.83940 2.81920\n",
       "[2,] 3.21181 3.19600 3.17942\n",
       "[3,] 2.58034 2.56404 2.54816\n",
       "[4,] 2.83672 2.81617 2.79622\n",
       "[5,] 3.17770 3.15770 3.13753\n",
       "[6,] 3.48325 3.46870 3.45307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(absorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>60.5</td><td>22.5</td><td>16.7</td></tr>\n",
       "\t<tr><td>46.0</td><td>40.1</td><td>13.5</td></tr>\n",
       "\t<tr><td>71.0</td><td> 8.4</td><td>20.5</td></tr>\n",
       "\t<tr><td>72.8</td><td> 5.9</td><td>20.7</td></tr>\n",
       "\t<tr><td>58.3</td><td>25.5</td><td>15.5</td></tr>\n",
       "\t<tr><td>44.0</td><td>42.7</td><td>13.7</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       "\t 60.5 & 22.5 & 16.7\\\\\n",
       "\t 46.0 & 40.1 & 13.5\\\\\n",
       "\t 71.0 &  8.4 & 20.5\\\\\n",
       "\t 72.8 &  5.9 & 20.7\\\\\n",
       "\t 58.3 & 25.5 & 15.5\\\\\n",
       "\t 44.0 & 42.7 & 13.7\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 60.5 | 22.5 | 16.7 | \n",
       "| 46.0 | 40.1 | 13.5 | \n",
       "| 71.0 |  8.4 | 20.5 | \n",
       "| 72.8 |  5.9 | 20.7 | \n",
       "| 58.3 | 25.5 | 15.5 | \n",
       "| 44.0 | 42.7 | 13.7 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3]\n",
       "[1,] 60.5 22.5 16.7\n",
       "[2,] 46.0 40.1 13.5\n",
       "[3,] 71.0  8.4 20.5\n",
       "[4,] 72.8  5.9 20.7\n",
       "[5,] 58.3 25.5 15.5\n",
       "[6,] 44.0 42.7 13.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:215, 1:100] 2.62 2.83 2.58 2.82 2.79 ...\n"
     ]
    }
   ],
   "source": [
    "str(absorp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:215, 1:3] 60.5 46 71 72.8 58.3 44 44 69.3 61.4 61.4 ...\n"
     ]
    }
   ],
   "source": [
    "str(endpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 이 예제에서 예측 변수는 각각의 빈도에 대한 측정값이다. 빈도는 체계적인 순서대로 들어 있다 보니(850 ~ 1,050nm), 예측 변수 간에 높은 상관성이 나타난다. 따라서 예측 변수의 개수(215개)보다 적은 차원의 데이터를 사용한다. PCA를 통해 이 데이터의 효율적인 차원의 수를 판단해 보자. **효율적인 차원**은 얼마인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p <- prcomp(absorp, center= T, scale = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctVar <- p$sdev^2 / sum(p$sdev^2) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>98.6261925818027</li>\n",
       "\t<li>0.969705228604993</li>\n",
       "\t<li>0.279324275947817</li>\n",
       "\t<li>0.11442986809507</li>\n",
       "\t<li>0.00646091116817935</li>\n",
       "\t<li>0.00262459145496293</li>\n",
       "\t<li>0.000718714169267827</li>\n",
       "\t<li>0.000384504977269123</li>\n",
       "\t<li>7.33460687485875e-05</li>\n",
       "\t<li>4.54197409037644e-05</li>\n",
       "\t<li>1.97304602563891e-05</li>\n",
       "\t<li>1.12953235577619e-05</li>\n",
       "\t<li>3.48639121036644e-06</li>\n",
       "\t<li>1.89495503827386e-06</li>\n",
       "\t<li>8.9288483394737e-07</li>\n",
       "\t<li>7.46658910773093e-07</li>\n",
       "\t<li>5.71246768446454e-07</li>\n",
       "\t<li>4.86796913884462e-07</li>\n",
       "\t<li>3.46155779741696e-07</li>\n",
       "\t<li>2.14170679624977e-07</li>\n",
       "\t<li>1.51832407536614e-07</li>\n",
       "\t<li>1.11654905400269e-07</li>\n",
       "\t<li>9.75160809184504e-08</li>\n",
       "\t<li>7.40610066913394e-08</li>\n",
       "\t<li>6.84162327339785e-08</li>\n",
       "\t<li>4.45107907525924e-08</li>\n",
       "\t<li>3.81878581648632e-08</li>\n",
       "\t<li>3.44967903007096e-08</li>\n",
       "\t<li>2.99058452797594e-08</li>\n",
       "\t<li>2.74301859999765e-08</li>\n",
       "\t<li>2.36761833504061e-08</li>\n",
       "\t<li>2.16941361854138e-08</li>\n",
       "\t<li>1.93850821943461e-08</li>\n",
       "\t<li>1.79272328344176e-08</li>\n",
       "\t<li>1.60957566660899e-08</li>\n",
       "\t<li>1.17048752502039e-08</li>\n",
       "\t<li>1.08221586262961e-08</li>\n",
       "\t<li>9.95913168536454e-09</li>\n",
       "\t<li>8.24691065669733e-09</li>\n",
       "\t<li>7.51316517161055e-09</li>\n",
       "\t<li>6.44205881052112e-09</li>\n",
       "\t<li>6.02478534504438e-09</li>\n",
       "\t<li>5.4172740148169e-09</li>\n",
       "\t<li>4.63458745756035e-09</li>\n",
       "\t<li>4.27814508611665e-09</li>\n",
       "\t<li>4.14750222842852e-09</li>\n",
       "\t<li>3.4775445489099e-09</li>\n",
       "\t<li>2.94027380211422e-09</li>\n",
       "\t<li>2.52677320415893e-09</li>\n",
       "\t<li>2.3941061115305e-09</li>\n",
       "\t<li>2.12340603457913e-09</li>\n",
       "\t<li>1.95266060641477e-09</li>\n",
       "\t<li>1.62997204777508e-09</li>\n",
       "\t<li>1.48567840710507e-09</li>\n",
       "\t<li>1.44377056661609e-09</li>\n",
       "\t<li>1.3247947416208e-09</li>\n",
       "\t<li>1.22260330369359e-09</li>\n",
       "\t<li>1.18514643243603e-09</li>\n",
       "\t<li>1.06519411544619e-09</li>\n",
       "\t<li>9.63720219631058e-10</li>\n",
       "\t<li>9.23974471302626e-10</li>\n",
       "\t<li>8.75290457742966e-10</li>\n",
       "\t<li>8.089090455321e-10</li>\n",
       "\t<li>7.28723291174581e-10</li>\n",
       "\t<li>6.68937851918815e-10</li>\n",
       "\t<li>5.70184571610442e-10</li>\n",
       "\t<li>5.59055396989165e-10</li>\n",
       "\t<li>5.21816453268303e-10</li>\n",
       "\t<li>4.72085135473064e-10</li>\n",
       "\t<li>4.23616392117791e-10</li>\n",
       "\t<li>3.98881523643063e-10</li>\n",
       "\t<li>3.72643530249714e-10</li>\n",
       "\t<li>3.43693581159981e-10</li>\n",
       "\t<li>3.26582530304138e-10</li>\n",
       "\t<li>2.98540774304106e-10</li>\n",
       "\t<li>2.86672315834561e-10</li>\n",
       "\t<li>2.59860515086109e-10</li>\n",
       "\t<li>2.46182057904017e-10</li>\n",
       "\t<li>2.29960256351495e-10</li>\n",
       "\t<li>2.0893243752363e-10</li>\n",
       "\t<li>1.98373676842737e-10</li>\n",
       "\t<li>1.83853567552078e-10</li>\n",
       "\t<li>1.62584767177735e-10</li>\n",
       "\t<li>1.49768558676013e-10</li>\n",
       "\t<li>1.38790357394793e-10</li>\n",
       "\t<li>1.18858992910204e-10</li>\n",
       "\t<li>1.09160502607095e-10</li>\n",
       "\t<li>1.01866670639949e-10</li>\n",
       "\t<li>8.82880016875596e-11</li>\n",
       "\t<li>7.6174499702301e-11</li>\n",
       "\t<li>6.83934354696987e-11</li>\n",
       "\t<li>5.79606966304559e-11</li>\n",
       "\t<li>4.66478007783709e-11</li>\n",
       "\t<li>4.07458477736139e-11</li>\n",
       "\t<li>3.5351873802125e-11</li>\n",
       "\t<li>3.00084191351481e-11</li>\n",
       "\t<li>2.32924617955163e-11</li>\n",
       "\t<li>2.04361992309639e-11</li>\n",
       "\t<li>1.73351943687466e-11</li>\n",
       "\t<li>1.69874931695463e-11</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 98.6261925818027\n",
       "\\item 0.969705228604993\n",
       "\\item 0.279324275947817\n",
       "\\item 0.11442986809507\n",
       "\\item 0.00646091116817935\n",
       "\\item 0.00262459145496293\n",
       "\\item 0.000718714169267827\n",
       "\\item 0.000384504977269123\n",
       "\\item 7.33460687485875e-05\n",
       "\\item 4.54197409037644e-05\n",
       "\\item 1.97304602563891e-05\n",
       "\\item 1.12953235577619e-05\n",
       "\\item 3.48639121036644e-06\n",
       "\\item 1.89495503827386e-06\n",
       "\\item 8.9288483394737e-07\n",
       "\\item 7.46658910773093e-07\n",
       "\\item 5.71246768446454e-07\n",
       "\\item 4.86796913884462e-07\n",
       "\\item 3.46155779741696e-07\n",
       "\\item 2.14170679624977e-07\n",
       "\\item 1.51832407536614e-07\n",
       "\\item 1.11654905400269e-07\n",
       "\\item 9.75160809184504e-08\n",
       "\\item 7.40610066913394e-08\n",
       "\\item 6.84162327339785e-08\n",
       "\\item 4.45107907525924e-08\n",
       "\\item 3.81878581648632e-08\n",
       "\\item 3.44967903007096e-08\n",
       "\\item 2.99058452797594e-08\n",
       "\\item 2.74301859999765e-08\n",
       "\\item 2.36761833504061e-08\n",
       "\\item 2.16941361854138e-08\n",
       "\\item 1.93850821943461e-08\n",
       "\\item 1.79272328344176e-08\n",
       "\\item 1.60957566660899e-08\n",
       "\\item 1.17048752502039e-08\n",
       "\\item 1.08221586262961e-08\n",
       "\\item 9.95913168536454e-09\n",
       "\\item 8.24691065669733e-09\n",
       "\\item 7.51316517161055e-09\n",
       "\\item 6.44205881052112e-09\n",
       "\\item 6.02478534504438e-09\n",
       "\\item 5.4172740148169e-09\n",
       "\\item 4.63458745756035e-09\n",
       "\\item 4.27814508611665e-09\n",
       "\\item 4.14750222842852e-09\n",
       "\\item 3.4775445489099e-09\n",
       "\\item 2.94027380211422e-09\n",
       "\\item 2.52677320415893e-09\n",
       "\\item 2.3941061115305e-09\n",
       "\\item 2.12340603457913e-09\n",
       "\\item 1.95266060641477e-09\n",
       "\\item 1.62997204777508e-09\n",
       "\\item 1.48567840710507e-09\n",
       "\\item 1.44377056661609e-09\n",
       "\\item 1.3247947416208e-09\n",
       "\\item 1.22260330369359e-09\n",
       "\\item 1.18514643243603e-09\n",
       "\\item 1.06519411544619e-09\n",
       "\\item 9.63720219631058e-10\n",
       "\\item 9.23974471302626e-10\n",
       "\\item 8.75290457742966e-10\n",
       "\\item 8.089090455321e-10\n",
       "\\item 7.28723291174581e-10\n",
       "\\item 6.68937851918815e-10\n",
       "\\item 5.70184571610442e-10\n",
       "\\item 5.59055396989165e-10\n",
       "\\item 5.21816453268303e-10\n",
       "\\item 4.72085135473064e-10\n",
       "\\item 4.23616392117791e-10\n",
       "\\item 3.98881523643063e-10\n",
       "\\item 3.72643530249714e-10\n",
       "\\item 3.43693581159981e-10\n",
       "\\item 3.26582530304138e-10\n",
       "\\item 2.98540774304106e-10\n",
       "\\item 2.86672315834561e-10\n",
       "\\item 2.59860515086109e-10\n",
       "\\item 2.46182057904017e-10\n",
       "\\item 2.29960256351495e-10\n",
       "\\item 2.0893243752363e-10\n",
       "\\item 1.98373676842737e-10\n",
       "\\item 1.83853567552078e-10\n",
       "\\item 1.62584767177735e-10\n",
       "\\item 1.49768558676013e-10\n",
       "\\item 1.38790357394793e-10\n",
       "\\item 1.18858992910204e-10\n",
       "\\item 1.09160502607095e-10\n",
       "\\item 1.01866670639949e-10\n",
       "\\item 8.82880016875596e-11\n",
       "\\item 7.6174499702301e-11\n",
       "\\item 6.83934354696987e-11\n",
       "\\item 5.79606966304559e-11\n",
       "\\item 4.66478007783709e-11\n",
       "\\item 4.07458477736139e-11\n",
       "\\item 3.5351873802125e-11\n",
       "\\item 3.00084191351481e-11\n",
       "\\item 2.32924617955163e-11\n",
       "\\item 2.04361992309639e-11\n",
       "\\item 1.73351943687466e-11\n",
       "\\item 1.69874931695463e-11\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 98.6261925818027\n",
       "2. 0.969705228604993\n",
       "3. 0.279324275947817\n",
       "4. 0.11442986809507\n",
       "5. 0.00646091116817935\n",
       "6. 0.00262459145496293\n",
       "7. 0.000718714169267827\n",
       "8. 0.000384504977269123\n",
       "9. 7.33460687485875e-05\n",
       "10. 4.54197409037644e-05\n",
       "11. 1.97304602563891e-05\n",
       "12. 1.12953235577619e-05\n",
       "13. 3.48639121036644e-06\n",
       "14. 1.89495503827386e-06\n",
       "15. 8.9288483394737e-07\n",
       "16. 7.46658910773093e-07\n",
       "17. 5.71246768446454e-07\n",
       "18. 4.86796913884462e-07\n",
       "19. 3.46155779741696e-07\n",
       "20. 2.14170679624977e-07\n",
       "21. 1.51832407536614e-07\n",
       "22. 1.11654905400269e-07\n",
       "23. 9.75160809184504e-08\n",
       "24. 7.40610066913394e-08\n",
       "25. 6.84162327339785e-08\n",
       "26. 4.45107907525924e-08\n",
       "27. 3.81878581648632e-08\n",
       "28. 3.44967903007096e-08\n",
       "29. 2.99058452797594e-08\n",
       "30. 2.74301859999765e-08\n",
       "31. 2.36761833504061e-08\n",
       "32. 2.16941361854138e-08\n",
       "33. 1.93850821943461e-08\n",
       "34. 1.79272328344176e-08\n",
       "35. 1.60957566660899e-08\n",
       "36. 1.17048752502039e-08\n",
       "37. 1.08221586262961e-08\n",
       "38. 9.95913168536454e-09\n",
       "39. 8.24691065669733e-09\n",
       "40. 7.51316517161055e-09\n",
       "41. 6.44205881052112e-09\n",
       "42. 6.02478534504438e-09\n",
       "43. 5.4172740148169e-09\n",
       "44. 4.63458745756035e-09\n",
       "45. 4.27814508611665e-09\n",
       "46. 4.14750222842852e-09\n",
       "47. 3.4775445489099e-09\n",
       "48. 2.94027380211422e-09\n",
       "49. 2.52677320415893e-09\n",
       "50. 2.3941061115305e-09\n",
       "51. 2.12340603457913e-09\n",
       "52. 1.95266060641477e-09\n",
       "53. 1.62997204777508e-09\n",
       "54. 1.48567840710507e-09\n",
       "55. 1.44377056661609e-09\n",
       "56. 1.3247947416208e-09\n",
       "57. 1.22260330369359e-09\n",
       "58. 1.18514643243603e-09\n",
       "59. 1.06519411544619e-09\n",
       "60. 9.63720219631058e-10\n",
       "61. 9.23974471302626e-10\n",
       "62. 8.75290457742966e-10\n",
       "63. 8.089090455321e-10\n",
       "64. 7.28723291174581e-10\n",
       "65. 6.68937851918815e-10\n",
       "66. 5.70184571610442e-10\n",
       "67. 5.59055396989165e-10\n",
       "68. 5.21816453268303e-10\n",
       "69. 4.72085135473064e-10\n",
       "70. 4.23616392117791e-10\n",
       "71. 3.98881523643063e-10\n",
       "72. 3.72643530249714e-10\n",
       "73. 3.43693581159981e-10\n",
       "74. 3.26582530304138e-10\n",
       "75. 2.98540774304106e-10\n",
       "76. 2.86672315834561e-10\n",
       "77. 2.59860515086109e-10\n",
       "78. 2.46182057904017e-10\n",
       "79. 2.29960256351495e-10\n",
       "80. 2.0893243752363e-10\n",
       "81. 1.98373676842737e-10\n",
       "82. 1.83853567552078e-10\n",
       "83. 1.62584767177735e-10\n",
       "84. 1.49768558676013e-10\n",
       "85. 1.38790357394793e-10\n",
       "86. 1.18858992910204e-10\n",
       "87. 1.09160502607095e-10\n",
       "88. 1.01866670639949e-10\n",
       "89. 8.82880016875596e-11\n",
       "90. 7.6174499702301e-11\n",
       "91. 6.83934354696987e-11\n",
       "92. 5.79606966304559e-11\n",
       "93. 4.66478007783709e-11\n",
       "94. 4.07458477736139e-11\n",
       "95. 3.5351873802125e-11\n",
       "96. 3.00084191351481e-11\n",
       "97. 2.32924617955163e-11\n",
       "98. 2.04361992309639e-11\n",
       "99. 1.73351943687466e-11\n",
       "100. 1.69874931695463e-11\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 9.862619e+01 9.697052e-01 2.793243e-01 1.144299e-01 6.460911e-03\n",
       "  [6] 2.624591e-03 7.187142e-04 3.845050e-04 7.334607e-05 4.541974e-05\n",
       " [11] 1.973046e-05 1.129532e-05 3.486391e-06 1.894955e-06 8.928848e-07\n",
       " [16] 7.466589e-07 5.712468e-07 4.867969e-07 3.461558e-07 2.141707e-07\n",
       " [21] 1.518324e-07 1.116549e-07 9.751608e-08 7.406101e-08 6.841623e-08\n",
       " [26] 4.451079e-08 3.818786e-08 3.449679e-08 2.990585e-08 2.743019e-08\n",
       " [31] 2.367618e-08 2.169414e-08 1.938508e-08 1.792723e-08 1.609576e-08\n",
       " [36] 1.170488e-08 1.082216e-08 9.959132e-09 8.246911e-09 7.513165e-09\n",
       " [41] 6.442059e-09 6.024785e-09 5.417274e-09 4.634587e-09 4.278145e-09\n",
       " [46] 4.147502e-09 3.477545e-09 2.940274e-09 2.526773e-09 2.394106e-09\n",
       " [51] 2.123406e-09 1.952661e-09 1.629972e-09 1.485678e-09 1.443771e-09\n",
       " [56] 1.324795e-09 1.222603e-09 1.185146e-09 1.065194e-09 9.637202e-10\n",
       " [61] 9.239745e-10 8.752905e-10 8.089090e-10 7.287233e-10 6.689379e-10\n",
       " [66] 5.701846e-10 5.590554e-10 5.218165e-10 4.720851e-10 4.236164e-10\n",
       " [71] 3.988815e-10 3.726435e-10 3.436936e-10 3.265825e-10 2.985408e-10\n",
       " [76] 2.866723e-10 2.598605e-10 2.461821e-10 2.299603e-10 2.089324e-10\n",
       " [81] 1.983737e-10 1.838536e-10 1.625848e-10 1.497686e-10 1.387904e-10\n",
       " [86] 1.188590e-10 1.091605e-10 1.018667e-10 8.828800e-11 7.617450e-11\n",
       " [91] 6.839344e-11 5.796070e-11 4.664780e-11 4.074585e-11 3.535187e-11\n",
       " [96] 3.000842e-11 2.329246e-11 2.043620e-11 1.733519e-11 1.698749e-11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pctVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1개의 주성분으로 전체 변동의 98.6%를 설명할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nQoIAIUGAkCBASBDwH2Ypgdp6e4ndAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"p\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "screeplot(p, npcs = 10, type = \"lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 데이터를 훈련 세트와 테스트 세트로 나누고 전처리한 후, 이 장에서 나온 다양한 모델을 적용해보자. 이 모델을 튜닝하는 경우, 튜닝 인수의 최적값은 얼마인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inMeatTraining <- createDataPartition(endpoints[,3], p = 3/4, list=FALSE) # Train 75%, Test 25%로 분할할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "absorpTrain <- absorp[inMeatTraining,]\n",
    "absorpTest <- absorp[-inMeatTraining,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "absorpTrain <- as.data.frame(absorpTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "absorpTest <- as.data.frame(absorpTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:163, 1:100] 2.62 2.83 2.58 2.82 2.79 ...\n"
     ]
    }
   ],
   "source": [
    "str(absorpTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinTrain <- endpoints[inMeatTraining,3]\n",
    "proteinTest <- endpoints[-inMeatTraining,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num [1:163] 16.7 13.5 20.5 20.7 15.5 13.7 13.7 19.3 17.7 17.7 ...\n"
     ]
    }
   ],
   "source": [
    "str(proteinTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl <- trainControl(method=\"repeatedcv\", number=10, repeats = 5) # 10-fold CV를 5번 반복하여 가장 좋은 후보의 파라미터 그리드를 찾게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모형 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "meatLM <- train(x = absorpTrain, y = proteinTrain, method = \"lm\", trControl = ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 147, 148, 146, 147, 147, 146, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE      \n",
       "  1.430337  0.8074329  0.9314441\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "meatPLS <- train(x = absorpTrain, y = proteinTrain, \n",
    "                 method = \"pls\", \n",
    "                 trControl = ctrl,\n",
    "                tuneLength = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partial Least Squares \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 146, 146, 147, 148, 147, 146, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  ncomp  RMSE       Rsquared   MAE      \n",
       "   1     2.8638198  0.1355567  2.4464685\n",
       "   2     2.1989272  0.4729260  1.7392220\n",
       "   3     1.7370406  0.6627022  1.2768824\n",
       "   4     1.5333060  0.7314303  1.1697324\n",
       "   5     1.1920076  0.8504575  0.9634359\n",
       "   6     1.1059376  0.8712577  0.9074462\n",
       "   7     1.0569052  0.8813774  0.8605135\n",
       "   8     0.9696922  0.9005839  0.7903158\n",
       "   9     0.9055291  0.9172822  0.7329252\n",
       "  10     0.8563940  0.9250247  0.6809880\n",
       "  11     0.7647464  0.9372350  0.6109695\n",
       "  12     0.6861870  0.9531565  0.5382337\n",
       "  13     0.6707836  0.9546850  0.5281806\n",
       "  14     0.6642074  0.9552055  0.5208182\n",
       "  15     0.6721212  0.9534079  0.5180098\n",
       "  16     0.6962017  0.9499978  0.5278978\n",
       "  17     0.7051798  0.9484573  0.5346544\n",
       "  18     0.7352615  0.9383542  0.5368071\n",
       "  19     0.7840893  0.9273283  0.5549048\n",
       "  20     0.8141748  0.9227074  0.5678851\n",
       "  21     0.8407918  0.9180323  0.5792171\n",
       "  22     0.9410796  0.8926692  0.6153213\n",
       "  23     1.0206267  0.8725330  0.6443205\n",
       "  24     1.0119528  0.8705956  0.6355477\n",
       "  25     1.0051117  0.8760004  0.6365841\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was ncomp = 14."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatPLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "meatPCR <- train(x = absorpTrain, y = proteinTrain, \n",
    "                 method = \"pcr\", \n",
    "                 trControl = ctrl,\n",
    "                tuneLength = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Principal Component Analysis \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 146, 147, 147, 147, 147, 147, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  ncomp  RMSE       Rsquared   MAE      \n",
       "   1     2.8767630  0.1515187  2.4632912\n",
       "   2     2.6770895  0.2221257  2.1865695\n",
       "   3     2.2065888  0.4778164  1.7613655\n",
       "   4     1.5824157  0.7133553  1.1956511\n",
       "   5     1.4103653  0.7791445  1.1005661\n",
       "   6     1.1232770  0.8684015  0.9129093\n",
       "   7     1.1237314  0.8682899  0.9168145\n",
       "   8     1.1150692  0.8694036  0.9097456\n",
       "   9     1.0917091  0.8745625  0.8997649\n",
       "  10     0.9546760  0.9049362  0.7679942\n",
       "  11     0.9621105  0.9013715  0.7632311\n",
       "  12     0.8788768  0.9187650  0.7117832\n",
       "  13     0.7780074  0.9360704  0.6233728\n",
       "  14     0.7434190  0.9434553  0.5826568\n",
       "  15     0.6815905  0.9531430  0.5458448\n",
       "  16     0.6678576  0.9547202  0.5355242\n",
       "  17     0.6795819  0.9525293  0.5402814\n",
       "  18     0.7050629  0.9480363  0.5522476\n",
       "  19     0.6732956  0.9536879  0.5208240\n",
       "  20     0.6691071  0.9538961  0.5189011\n",
       "  21     0.6853685  0.9509352  0.5245288\n",
       "  22     0.6968422  0.9479353  0.5250550\n",
       "  23     0.6668026  0.9520839  0.5101888\n",
       "  24     0.6624675  0.9522655  0.5108139\n",
       "  25     0.6542629  0.9552815  0.5102542\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was ncomp = 25."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatPCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 15))\n",
    "\n",
    "meatRidge <- train(x = absorpTrain, y = proteinTrain, \n",
    "                   method = \"ridge\", \n",
    "                   tuneGrid = ridgeGrid,\n",
    "                   trControl = ctrl,\n",
    "                  preProc = c(\"center\", \"scale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge Regression \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "Pre-processing: centered (100), scaled (100) \n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 147, 147, 147, 147, 146, 147, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda       RMSE      Rsquared   MAE     \n",
       "  0.000000000  1.427514  0.8217638  0.935735\n",
       "  0.007142857  1.322822  0.8180611  1.030097\n",
       "  0.014285714  1.429359  0.7854807  1.094803\n",
       "  0.021428571  1.490275  0.7665356  1.130837\n",
       "  0.028571429  1.531852  0.7537476  1.156836\n",
       "  0.035714286  1.563526  0.7441646  1.177178\n",
       "  0.042857143  1.589432  0.7364373  1.194362\n",
       "  0.050000000  1.611640  0.7298726  1.209300\n",
       "  0.057142857  1.631296  0.7240821  1.223591\n",
       "  0.064285714  1.649081  0.7188331  1.237703\n",
       "  0.071428571  1.665425  0.7139790  1.252671\n",
       "  0.078571429  1.680617  0.7094233  1.267465\n",
       "  0.085714286  1.694855  0.7051002  1.281573\n",
       "  0.092857143  1.708286  0.7009636  1.295106\n",
       "  0.100000000  1.721017  0.6969805  1.307835\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was lambda = 0.007142857."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASSOGrid <- expand.grid(fraction = seq(.05, 1, length = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "meatLASSO <- train(x = absorpTrain, y = proteinTrain, \n",
    "                   method = \"lasso\",\n",
    "                   tuneGrid = LASSOGrid,\n",
    "                   trControl = ctrl,\n",
    "                  preProc = c(\"center\", \"scale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The lasso \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "Pre-processing: centered (100), scaled (100) \n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 147, 146, 147, 147, 147, 147, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  fraction  RMSE       Rsquared   MAE      \n",
       "  0.05      0.8258517  0.9125535  0.5551349\n",
       "  0.10      0.9455193  0.8856111  0.5938891\n",
       "  0.15      1.0092004  0.8726959  0.6261495\n",
       "  0.20      1.0484952  0.8643941  0.6482135\n",
       "  0.25      1.0916313  0.8561058  0.6714435\n",
       "  0.30      1.1488195  0.8457461  0.7035065\n",
       "  0.35      1.1908512  0.8401882  0.7313262\n",
       "  0.40      1.2274813  0.8357295  0.7582050\n",
       "  0.45      1.2582683  0.8318990  0.7817464\n",
       "  0.50      1.2844434  0.8284773  0.8002595\n",
       "  0.55      1.3064036  0.8255677  0.8172100\n",
       "  0.60      1.3322847  0.8215527  0.8355627\n",
       "  0.65      1.3595930  0.8173708  0.8542352\n",
       "  0.70      1.3866238  0.8132612  0.8737639\n",
       "  0.75      1.4143963  0.8088884  0.8941955\n",
       "  0.80      1.4452709  0.8041005  0.9155579\n",
       "  0.85      1.4752323  0.7994540  0.9362416\n",
       "  0.90      1.5071877  0.7943570  0.9575869\n",
       "  0.95      1.5383195  0.7893540  0.9781073\n",
       "  1.00      1.5686720  0.7844754  0.9983679\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was fraction = 0.05."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatLASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "enetGrid <- expand.grid(lambda = c(0, 0.01, .1), \n",
    "                        fraction = seq(.05, 1, length = 20))\n",
    "\n",
    "meatEnet <- train(x = absorpTrain, y = proteinTrain, \n",
    "                   method = \"enet\", \n",
    "                   tuneGrid = enetGrid,\n",
    "                   trControl = ctrl,\n",
    "                  preProc = c(\"center\", \"scale\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elasticnet \n",
       "\n",
       "163 samples\n",
       "100 predictors\n",
       "\n",
       "Pre-processing: centered (100), scaled (100) \n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 147, 146, 147, 147, 146, 146, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  lambda  fraction  RMSE       Rsquared   MAE      \n",
       "  0.00    0.05      0.7982863  0.9243982  0.5458103\n",
       "  0.00    0.10      0.9252634  0.8958645  0.5895239\n",
       "  0.00    0.15      0.9923364  0.8828655  0.6244370\n",
       "  0.00    0.20      1.0387343  0.8730811  0.6462747\n",
       "  0.00    0.25      1.0830868  0.8659595  0.6723427\n",
       "  0.00    0.30      1.1232659  0.8613733  0.6974709\n",
       "  0.00    0.35      1.1598334  0.8559937  0.7216308\n",
       "  0.00    0.40      1.1868522  0.8519091  0.7432981\n",
       "  0.00    0.45      1.2049488  0.8494618  0.7597723\n",
       "  0.00    0.50      1.2233830  0.8456956  0.7758998\n",
       "  0.00    0.55      1.2414911  0.8425519  0.7903421\n",
       "  0.00    0.60      1.2622063  0.8390370  0.8051295\n",
       "  0.00    0.65      1.2850283  0.8350404  0.8208098\n",
       "  0.00    0.70      1.3087004  0.8311230  0.8355930\n",
       "  0.00    0.75      1.3339591  0.8269805  0.8512081\n",
       "  0.00    0.80      1.3591772  0.8230241  0.8668000\n",
       "  0.00    0.85      1.3860064  0.8185600  0.8834554\n",
       "  0.00    0.90      1.4131605  0.8136673  0.9006160\n",
       "  0.00    0.95      1.4404126  0.8083606  0.9177847\n",
       "  0.00    1.00      1.4683345  0.8030979  0.9358112\n",
       "  0.01    0.05      2.6892737  0.2696942  2.3026010\n",
       "  0.01    0.10      2.4889188  0.4340911  2.1275106\n",
       "  0.01    0.15      2.2984489  0.5570607  1.9601878\n",
       "  0.01    0.20      2.1222294  0.6354442  1.7995450\n",
       "  0.01    0.25      1.9616635  0.6830102  1.6443511\n",
       "  0.01    0.30      1.8208706  0.7116607  1.4973798\n",
       "  0.01    0.35      1.7046382  0.7293157  1.3663648\n",
       "  0.01    0.40      1.6178321  0.7406593  1.2679170\n",
       "  0.01    0.45      1.5621915  0.7486635  1.2062174\n",
       "  0.01    0.50      1.5279403  0.7562314  1.1730313\n",
       "  0.01    0.55      1.5005481  0.7632947  1.1505850\n",
       "  0.01    0.60      1.4795210  0.7695878  1.1344436\n",
       "  0.01    0.65      1.4602947  0.7756995  1.1208690\n",
       "  0.01    0.70      1.4421182  0.7816117  1.1084781\n",
       "  0.01    0.75      1.4255288  0.7869978  1.0974396\n",
       "  0.01    0.80      1.4104372  0.7918656  1.0874688\n",
       "  0.01    0.85      1.3968190  0.7962448  1.0785616\n",
       "  0.01    0.90      1.3846309  0.8001469  1.0705916\n",
       "  0.01    0.95      1.3750170  0.8032012  1.0642416\n",
       "  0.01    1.00      1.3689352  0.8050769  1.0601737\n",
       "  0.10    0.05      2.7906626  0.1802340  2.3814114\n",
       "  0.10    0.10      2.6905873  0.2542249  2.2914785\n",
       "  0.10    0.15      2.5931809  0.3290305  2.2064267\n",
       "  0.10    0.20      2.4995509  0.3980085  2.1248653\n",
       "  0.10    0.25      2.4098422  0.4579573  2.0457632\n",
       "  0.10    0.30      2.3246585  0.5078096  1.9692238\n",
       "  0.10    0.35      2.2433432  0.5486008  1.8938852\n",
       "  0.10    0.40      2.1669401  0.5810230  1.8209265\n",
       "  0.10    0.45      2.0956919  0.6067944  1.7507643\n",
       "  0.10    0.50      2.0300824  0.6271492  1.6835418\n",
       "  0.10    0.55      1.9695917  0.6434889  1.6189875\n",
       "  0.10    0.60      1.9148906  0.6564762  1.5573147\n",
       "  0.10    0.65      1.8664697  0.6668476  1.5012354\n",
       "  0.10    0.70      1.8249787  0.6751321  1.4522309\n",
       "  0.10    0.75      1.7911365  0.6817899  1.4118251\n",
       "  0.10    0.80      1.7650504  0.6867256  1.3802762\n",
       "  0.10    0.85      1.7444968  0.6906469  1.3555305\n",
       "  0.10    0.90      1.7281438  0.6935983  1.3355363\n",
       "  0.10    0.95      1.7143007  0.6960854  1.3183633\n",
       "  0.10    1.00      1.7031173  0.6980256  1.3035560\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were fraction = 0.05 and lambda = 0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meatEnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 어느 모델의 예측력이 가장 좋은가?\n",
    "\n",
    "- Elastic Net의 lambda = 0, fraction = 0.05 일 때 RMSE = 0.798로 가장 낮다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
